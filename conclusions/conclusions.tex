\chapter{General conclusions} \label{ch:conclusions}
This dissertation has summarized five contributions related to the low-resource deep learning field, showing how improvements of state of the art can be made without necessarily needing to spend a prohibitive amount of resources. 

\begin{itemize}
\item The modulus activation function, introduced in chapter \ref{ch:modulus}, is a living example of the premise of this thesis, showing that a bit-size operation can outperform the most novel and complex activation functions in 75\% of the experiments conducted, despite its simplicity. The modulus activation function computational cost is equivalent to the \textit{ReLU}'s, but its constant gradient norm guarantees a better utilization of the network parameters, removing the ``dying neurons'' problem, often leading to more efficient training processes.

\item Transfer learning also showed recently that impressive results can be achieved by just fine-tuning a few thousands of the millions of weights of a large network which has been previously trained on a general pretext task. Knowledge distillation techniques allow leveraging the knowledge of big networks to implant it into small ones. Combining both techniques has been a clear objective for the study line of this thesis. Chapter \ref{ch:distillation} provides evidences on how it is possible to increase the accuracy of a small pretrained neural network up to 3\% (in absolute terms), by just learning from their large counterparts and using solely unlabeled data. This case study motivates the idea that the small neural networks are not yet at their capacity limit. Based on the results of this study, a possible future research line may be looking for more efficient learning techniques that enable the use all the modeling capacity of the parameters of a neural network.

\item Thinking of applications, the outstanding generalization capacity of deep learning models, combined with the flexibility and customizability of their architecture, enables one to design models that solve many tasks at once. One of the main benefits of having a single model, as opposed to train standalone models for each individual task, is that the amount of total parameters, the effort required and the computational cost of tuning the hyper-parameters is smaller than tackling each problem with a different model. Besides, recent studies have shown that when a neural network is asked to perform many tasks, the overall performance increases \autocite{Jaderberg2016} as if the model had more appeal to learn. These ideas motivated the design of the the sales forecasting problem as an end-to-end solution, as described in chapter \ref{ch:salesforecast}. It describes how to train a single model to predict the daily number of sales each of 4400 items in 54 different points of sale, and show that the proposed solution is better than all the published benchmarks. In a production environment, this means that a company like \textit{Corporaci√≥n Favorita}, the owners of the open-source data set used in the experiments, would be able to deploy the model in a single machine which would serve all their points of sale (as opposed of having to build 4400 products $\times$ 54 points of sale $=$ 237600 standalone models). Overall, this would result in lower infrastructure and computational costs.

\item Speech technologies also got benefited by the fast development of deep learning technologies of the last years. Generally speaking, the computational speech field is divided in two branches: speech recognition and speech generation; deep learning has revolutionized both of them. 
	\begin{itemize}
		\item The contribution to the first branch, developed in chapter \ref{ch:kws} in form of a Keyword Spotting model, was motivated by the outstanding performance of CNNs in the computer vision field. An architecture designed for computer vision tasks has been adapted to the problem of keyword spotting, achieving state of the art results when compared with all the reported benchmarks. The \textit{Xception} architecture is designed to work with depthwise-separable convolutions, which is an efficient modification of the classical convolution operations used in deep learning (as shown in section \ref{sec:kwsdwscost}). Along with this work comes the design and implementation of a (i) \textit{depthwise separable} CNN-based architecture able to surpass the current state of the art results and the human performance, (ii) the development of a methodology for augmenting audio data to increase the size of a data set (5x in this work), and therefore enhance generalization, (iii) the quantification of the human performance across the different classification tasks to use it as an additional baseline for checking the results achieved by the algorithm and (iv) the creation of a public repository where further contributions could be handled to enhance the project functionalities and to facilitate reproducibility.
		\item Chapter \ref{ch:tts} contributes to the speech generation field by proposing a modification to a baseline architecture that brings computational and quality improvements in form of prosody variation. The computational improvements come from the dependency of the baseline architecture on a production speech generation system: for generating a new audio clip, the baseline model needed to get an example of that utterance as input as a reference for it to work. For that, a commercial solution is normally used (e.g. \textit{Amazon Polly}). The proposed architecture replaces the reference encoder by a residual branch and a normalizing flow, which together build a new TTS architecture that, as empirically proved, generates syntheses with more varied prosody. The proposed architecture is capable of working on a single offline computer, not depending on a production TTS service. 
	\end{itemize}
\end{itemize}

The potential of deep learning has not yet been fully explored. The low-resource deep learning field has recently started to gain traction and it is expected to snag the attention of many researchers and practitioners in the coming years. As the momentum grows, the amount and variety of problems that will be solved using deep learning will also increase, bringing new challenges with it. Looking back at the work reported in this dissertation, relevant contributions have been made to the research field focused on lowering the resources requirements of the deep learning modeling efforts: two general contributions and three examples showing that many problems can be solved while using a reasonable use of resources. This is a very important area of study, because lowering the bar of the resources requirements will the door to the use of deep learning technologies in a larger set of use cases and application domains.  The area of low-resource deep learning will probably be one of the most important areas of research in the coming years, and hopefully the work reported in this dissertation will contribute to the advancement of this area of study.

