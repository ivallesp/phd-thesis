\chapter{Conclusions} \label{ch:conclusions}
This dissertation has provided five contributions related to the low-resource deep learning field. We have shown how one can make improvements of state of the art techniques without necessarily needing to spend more resources. 

The modulus activation function, introduced in chapter \ref{ch:modulus}, is a living example of the premise of this thesis, showing that a bit-size operation can outperform the most novel and complex activation functions in 75\% of the experiments conducted, despite its simplicity. The modulus activation function computational cost is equivalent to the \textit{ReLU}'s, but its constant gradient norm guarantees a better utilization of the network parameters, removing the ``dying neurons'' problem. 

Transfer learning also showed recently that impressive results can be achieved by just fine-tuning a few thousands of the millions of weights of a large network which was previously trained on a general task. Knowledge distillation techniques allow us leveraging the knowledge of big networks to implant it into small ones. Combining both techniques was a clear objective for the study line of this thesis. As we showed in chapter \ref{ch:distillation}, it is possible to increase the accuracy of a small pretrained neural network up to 3\% (in absolute terms), by just learning from their large counterparts. This case study motivates us to think that the small neural networks are not yet at their capacity limit. Based on the results of this study, we think a possible future research line may be looking for more efficient learning techniques that allow us to use all the modeling capacity of the parameters of a neural network.

Thinking of applications, the outstanding generalization capacity of deep learning models, combined with the flexibility and customizability of their architecture, enables one to design models that solve many tasks at once. One of the main benefits of having a single model, as opposed to train standalone models for each individual task, is that the amount of total parameters, the effort required and the computational cost of tuning the hyper-parameters is smaller than tackling each problem with a different model. Besides, recent studies have shown that when a neural network is asked to perform many tasks, the overall performance increases as if the model had more appeal to learn. These ideas motivated us to tackle the sales forecasting problem from an end-to-end perspective in chapter \ref{ch:salesforecast}. Here we propose to learn a single model to predict the daily number of sales each of 4400 items in 54 different points of sale, and show that our solution is better than all the published benchmarks. In a production environment, this means that a company like \textit{Corporaci√≥n Favorita}, the owners of the open-source data set used in the experiments, would be able to deploy our model in a single machine which would serve all their points of sale. Overall, this would result in lower infrastructure and computational costs.

Speech technologies also got benefited by the fast development of deep learning technologies of the last years. Generally speaking, the computational speech field is divided in two branches: speech recognition and speech generation; deep learning has revolutionized both of them. Our contribution to the first branch, developed in chapter \ref{ch:kws} in form of a Keyword Spotting model, was motivated by the outstanding performance of CNNs in the computer vision field. We adapted an architecture designed for computer vision tasks to the problem of keyword spotting, achieving state of the art results when compared with all the reported benchmarks. The \textit{Xception} architecture is designed to work with depthwise-separable convolutions, which is an efficient modification of the classical convolution operations used in deep learning (as shown in section \ref{sec:kwsdwscost}). In chapter \ref{ch:tts} we contribute to the speech generation field by proposing a modification to a baseline architecture that brings computational and quality improvements in form of prosody variation. The computational improvements come from the dependency of the baseline architecture on a production speech generation system: for generating a new audio clip, the baseline model needed to get an example of that utterance as input as a reference for it to work. For that, a commercial solution is normally used (e.g. \textit{Amazon Polly}). The proposed architecture replaces the reference encoder by a residual branch and a normalizing flow, which together build a new TTS architecture that, as we proved, generates syntheses with more varied prosody. The proposed architecture is capable of working on a single offline computer, not depending on a production TTS service.

Looking back at the work reported in this dissertation, we consider that we have done a relevant contribution to the research field focused on lowering the resources requirements of the deep learning modeling efforts. We have made two general contributions to that line, and have provided three examples showing that many problems can be solved while using a reasonable use of resources. We think that our collective efforts towards Artificial General Intelligence (AGI) should be rooted in these principles in order to guarantee the safety of our planet environment as well as democratizing the artificial intelligence as tool that is accessible to everyone. 


