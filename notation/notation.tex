%!TEX root = ../thesis.tex
\chapter*{Notation} \label{ch:notation}
We tried to follow a common notation throughout this dissertation. This chapter provides a brief reference of all the conventions followed. 


\section*{Data}

\begin{labeling}{alligator}
	\item [$\mathbf{X}$] a matrix with shape $N \times D$ containing $N$ input row vectors, where each vector is represented as $\{\mathbf{x_1},...,\mathbf{x_i},...,\mathbf{x_N}\}$.
	\item [$\mathbf{x_i}$] a vector of length $D$ where each element represents a particular feature (e.g. age, weight, or IQ of a person).
	\item [$\mathbf{Y}$] a matrix with shape $N \times K$ containing desired output vectors row $\{\mathbf{y_1},...,\mathbf{y_i},...,\mathbf{y_K}\}$.
	\item [$\mathbf{y_i}$]  a vector of length $K$ where each element is one of the scalar desired outputs (e.g. the probability of having a high income in a 5 years time span).
	\item [$\mathbf{T}$] a data set  of examples with inputs and desired outputs for supervised learning tasks, composed of a set of pairs of vectors from $\mathbf{X}$ and $\mathbf{Y}$ matrices grouped as follows: $\{(\mathbf{x_1}, \mathbf{y_1})...,(\mathbf{x_i}, \mathbf{y_i}),...,(\mathbf{x_N}, \mathbf{y_N})\}$, where each tuple represents a training example.
	\item [$\mathbf{U}$] a data set of examples for unsupervised learning tasks, composed of a set of input vectors from $\mathbf{X}$ disposed as follows: $\{\mathbf{x_1},...,\mathbf{x_i},...,\mathbf{x_N}\}$, where each element represents a training example.
\end{labeling}

\section*{Neural networks and machine learning}

\begin{labeling}{alligator}
	\item [$\mathbf{\theta}$] vector containing all the parameters of a neural netword model.
	\item [$f_\mathbf{\theta}(\cdot)$] neural network model with parameters $\mathbf{\theta}$.
	\item [$J(\cdot, \cdot)$] cost function, i.e. differentiable function quantifying the error to be minimized, normally using gradient descent.
	\item [$\mathbf{W}$] weights matrix of a neural network layer, which scalar components are denoted by $w_{ij}$.
	\item[$g(\cdot)$] activation function.
	\item[$G(\cdot)$] multi-layer Perceptron.
	\item[$t$] optimization step.
	\item[$\lambda$] learning rate.
	\item[$m$] minibatch size.
	\item[$L$] number of layers of a deep learning model.
	\item[$\mathbf{h}$] output of a hidden layer.
\end{labeling}


\section*{Calculus}

\section*{Others}
\begin{labeling}{alligator}
	\item [$H$] \textit{Heavyside} function.
	\item [$\sigma(\cdot)$] \textit{Sigmoid} function.
	\item [$\sigma(\tau)$] Hyperbolic tangent function ($\tanh(\cdot)$).
	
\end{labeling}